{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import reshape\n",
    "from tensorflow.python.keras import Input\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import LeakyReLU, Dense, Dropout, MaxPooling1D, Conv1D, Flatten, concatenate\n",
    "from tensorflow.python.keras.metrics import Precision, Recall\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "from tensorflow.python.keras.regularizers import L1L2, L2\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\"\"\"\n",
    "    Creates models consisting of two autoencoders and a CNN\n",
    "\"\"\"\n",
    "\n",
    "def trainSAE_CNN(x_train_norm, x_train_mal, x_train, y_train, nb_epoch_cnn, nb_epoch_sae, batch_size_cnn, batch_size_sae, datenow):\n",
    "    print(x_train.shape[1])\n",
    "    input_dim = x_train.shape[1]\n",
    "    act_reg = L1L2()\n",
    "    act = LeakyReLU()\n",
    "    hidden_dim_1 = input_dim\n",
    "    hidden_dim_2 = input_dim\n",
    "    optimizer = Adam()  # learning_rate=0.001)\n",
    "\n",
    "    filter1 = 32\n",
    "    filter2 = 64\n",
    "    filter3 = 128\n",
    "\n",
    "    # Autoencoder Normal\n",
    "    input_layer_aen = Input(shape=(input_dim,))\n",
    "    encoder_norm = Dense(hidden_dim_1, activation=act, use_bias=False, activity_regularizer=act_reg, name=\"hl_norm1\")(\n",
    "        input_layer_aen)\n",
    "    encoder_norm = BatchNormalization()(encoder_norm)\n",
    "    encoder_norm = Dropout(0.5)(encoder_norm)\n",
    "\n",
    "    encoder_norm = Dense(hidden_dim_2, activation=act, use_bias=False, activity_regularizer=act_reg, name=\"hl_norm2\")(\n",
    "        encoder_norm)\n",
    "    encoder_norm = BatchNormalization()(encoder_norm)\n",
    "    encoder_norm = Dropout(0.5)(encoder_norm)\n",
    "\n",
    "    decoder_norm = Dense(hidden_dim_1, activation=act, activity_regularizer=act_reg)(encoder_norm)  # or softmax\n",
    "    decoder_norm = BatchNormalization()(decoder_norm)\n",
    "    decoder_norm = Dropout(0.5)(decoder_norm)\n",
    "\n",
    "    decoder_norm = Dense(input_dim, activation='sigmoid')(decoder_norm)  # or softmax\n",
    "\n",
    "    autoencoder_norm = Model(inputs=input_layer_aen, outputs=decoder_norm, name=\"ae_norm\")\n",
    "\n",
    "    # Autoencoder Malicious\n",
    "    input_layer_aem = Input(shape=(input_dim,))\n",
    "    encoder_mal = Dense(hidden_dim_1, activation=act, use_bias=False, activity_regularizer=act_reg, name=\"hl_mal1\")(\n",
    "        input_layer_aem)\n",
    "    encoder_mal = BatchNormalization()(encoder_mal)\n",
    "    encoder_mal = Dropout(0.5)(encoder_mal)\n",
    "\n",
    "    encoder_mal = Dense(hidden_dim_2, activation=act, use_bias=False, activity_regularizer=act_reg, name=\"hl_mal2\")(\n",
    "        encoder_mal)\n",
    "    encoder_mal = BatchNormalization()(encoder_mal)\n",
    "    encoder_mal = Dropout(0.5)(encoder_mal)\n",
    "\n",
    "    decoder_mal = Dense(hidden_dim_1, activation=act, activity_regularizer=act_reg)(encoder_mal)  # or softmax\n",
    "    decoder_mal = BatchNormalization()(decoder_mal)\n",
    "    decoder_mal = Dropout(0.5)(decoder_mal)\n",
    "\n",
    "    decoder_mal = Dense(input_dim, activation='sigmoid')(decoder_mal)  # or softmax\n",
    "\n",
    "    autoencoder_mal = Model(inputs=input_layer_aem, outputs=decoder_mal, name=\"ae_mal\")\n",
    "\n",
    "    # compilation of AE_normal + AE_mal\n",
    "    autoencoder_norm.compile(metrics=['accuracy', Precision(), Recall()], loss='mse', optimizer=optimizer)\n",
    "    autoencoder_norm.summary()\n",
    "\n",
    "    autoencoder_mal.compile(metrics=['accuracy', Precision(), Recall()], loss='mse', optimizer=optimizer)\n",
    "    autoencoder_mal.summary()\n",
    "\n",
    "    history_norm = autoencoder_norm.fit(x_train_norm, x_train_norm, epochs=nb_epoch_sae, batch_size=batch_size_sae,\n",
    "                                        # validation_data=(x_test_norm, x_test_norm),\n",
    "                                        # callbacks=[EarlyStopping(monitor=\"val_loss\", patience=25, mode=\"min\")]\n",
    "                                        callbacks=[EarlyStopping(monitor=\"accuracy\", patience=25, mode=\"max\")]\n",
    "                                        )\n",
    "\n",
    "    history_mal = autoencoder_mal.fit(x_train_mal, x_train_mal, epochs=nb_epoch_sae, batch_size=batch_size_sae,\n",
    "                                      # validation_data=(x_test_mal, x_test_mal),\n",
    "                                      # callbacks=[EarlyStopping(monitor=\"val_loss\", patience=25, mode=\"min\")]\n",
    "                                      callbacks=[EarlyStopping(monitor=\"accuracy\", patience=25, mode=\"max\")]\n",
    "                                      )\n",
    "\n",
    "    inp = Input(shape=(input_dim,))\n",
    "\n",
    "    ae_norm_output = autoencoder_norm(inp)\n",
    "    ae_mal_output = autoencoder_mal(inp)\n",
    "\n",
    "    # concat_layer = concatenate([autoencoder_norm.get_layer(name=\"hl_norm2\").output, autoencoder_mal.get_layer(name=\"hl_mal2\").output], axis=-1)\n",
    "    concat_layer = concatenate([ae_norm_output, ae_mal_output], axis=-1)\n",
    "    concat_layer2 = reshape(concat_layer, [-1, concat_layer.shape[1], 1])\n",
    "\n",
    "    # CNN\n",
    "    y = Conv1D(filter1, 1, activation=act, activity_regularizer=act_reg, kernel_regularizer=L2(),\n",
    "               kernel_initializer='he_normal', input_shape=[input_dim, 1])(\n",
    "        concat_layer2)  # kernel_initializer=’he_normal’\n",
    "    y = Conv1D(filter1, 1, activation=act, activity_regularizer=act_reg, kernel_regularizer=L2(),\n",
    "               kernel_initializer='he_normal')(y)\n",
    "    y = MaxPooling1D(2, 2)(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "\n",
    "    y = Conv1D(filter2, 1, activation=act, activity_regularizer=act_reg, kernel_regularizer=L2(),\n",
    "               kernel_initializer='he_normal')(y)\n",
    "    y = Conv1D(filter2, 1, activation=act, activity_regularizer=act_reg, kernel_regularizer=L2(),\n",
    "               kernel_initializer='he_normal')(y)\n",
    "    y = MaxPooling1D(2, 2)(y)\n",
    "    # y = BatchNormalization(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "\n",
    "    y = Conv1D(filter3, 1, activation=act, activity_regularizer=act_reg, kernel_regularizer=L2(),\n",
    "               kernel_initializer='he_normal')(y)\n",
    "    y = Conv1D(filter3, 1, activation=act, activity_regularizer=act_reg, kernel_regularizer=L2(),\n",
    "               kernel_initializer='he_normal')(y)\n",
    "    y = MaxPooling1D(2, 2)(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(512)(y)\n",
    "    outputs = Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "    # cnn = Model(inputs=[input_layer_aen, input_layer_aem], outputs=outputs)\n",
    "    cnn = Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "    cnn.compile(metrics=['accuracy', Precision(), Recall()], optimizer=optimizer, loss='binary_crossentropy')\n",
    "    cnn.summary()\n",
    "\n",
    "    print(\"CNN training\")\n",
    "    history_cnn = cnn.fit(x=x_train, y=y_train, epochs=nb_epoch_cnn, shuffle=True, batch_size=batch_size_cnn,\n",
    "                          # validation_data=(x_test, y_test),\n",
    "                          # callbacks=[EarlyStopping(monitor=\"val_loss\", patience=25, mode=\"min\")])\n",
    "                          callbacks=[EarlyStopping(monitor=\"accuracy\", patience=int(nb_epoch_cnn / 2), mode=\"max\")])\n",
    "\n",
    "    print(\"Saving\")\n",
    "\n",
    "    cnn.save('./saved_models/sae_cnn_{}'.format(datenow.strftime(\"%Y-%m-%d_%H-%M-%S\")))\n",
    "\n",
    "    plot_model(cnn, to_file='./saved_models/model_sae_cnn_{}.png'.format(datenow.strftime(\"%Y-%m-%d_%H-%M-%S\")),\n",
    "               show_shapes=True, show_layer_names=True, expand_nested=True)\n",
    "\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('./datasets/Botnet_Train.csv', delimiter=\",\")\n",
    "train_data.drop(columns=['ip.session_id', 'meta.direction'], inplace=True)\n",
    "\n",
    "test_data = pd.read_csv('./datasets/Botnet_Test.csv', delimiter=\",\")\n",
    "test_data.drop(columns=['ip.session_id', 'meta.direction'], inplace=True)\n",
    "\n",
    "# reading the csv file\n",
    "print(\"Shape of the training data: %s\" % str(train_data.shape))\n",
    "print(\"Training labels: %s\" % (train_data['malware'].value_counts()))\n",
    "\n",
    "print(\"Shape of the testing data: %s\" % str(test_data.shape))\n",
    "print(\"Testing labels: %s\" % (test_data['malware'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "import numpy\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "#sys.path.append(sys.path[0] + '/..')\n",
    "from tools.tools import saveConfMatrix, saveScores, dataScale_cnn\n",
    "from model.sae_cnn import trainSAE_CNN\n",
    "\n",
    "\"\"\"\n",
    "    Runs tests (pretty much the same as runPredictions)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "d = datetime.now()\n",
    "x_train_norm, x_train_mal, x_test_norm, x_test_mal, x_train, y_train, x_test, y_test = dataScale_cnn(train_data,\n",
    "                                                                                                             test_data, datetime=d)\n",
    "\n",
    "# exec\n",
    "input_dim = x_train.shape[1]\n",
    "nb_epoch_sae = 5  # 30#10000\n",
    "batch_size_sae = 16  # 128\n",
    "nb_epoch_cnn = 2\n",
    "batch_size_cnn = 32\n",
    "\n",
    "cnn = trainSAE_CNN(x_train_norm=x_train_norm, x_train_mal=x_train_mal,\n",
    "                   x_train=x_train, y_train=y_train,\n",
    "                   nb_epoch_cnn=nb_epoch_cnn, nb_epoch_sae=nb_epoch_sae,\n",
    "                   batch_size_cnn=batch_size_cnn, batch_size_sae=batch_size_sae, datenow=d)\n",
    "\n",
    "print(\"Prediction - test\")\n",
    "y_pred = cnn.predict(x_test)\n",
    "y_pred = numpy.transpose(np.round(y_pred)).reshape(y_pred.shape[0], )\n",
    "\n",
    "print(\"Metrics\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "saveConfMatrix(y_true=y_test, y_pred=y_pred,\n",
    "               filepath_csv='./saved_results/sae-cnn/conf_matrix_sae_test-cnn_{}.csv'.format(d.strftime(\"%Y-%m-%d_%H-%M-%S\")),\n",
    "               filepath_png='./saved_results/sae-cnn/conf_matrix_sae-cnn_{}.jpg'.format(d.strftime(\"%Y-%m-%d_%H-%M-%S\")))\n",
    "saveScores(y_true=y_test, y_pred=y_pred, filepath='./saved_results/sae-cnn/stats_{}'.format(d.strftime(\"%Y-%m-%d_%H-%M-%S\")))\n",
    "\n",
    "preds = np.array([y_pred]).T\n",
    "res = np.append(x_test, preds, axis=1)\n",
    "pd.DataFrame(res).to_csv(\"./saved_results/sae-cnn/predictions_{}.csv\".format(d.strftime(\"%Y-%m-%d_%H-%M-%S\")), index=False,\n",
    "                         header=test_data.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
